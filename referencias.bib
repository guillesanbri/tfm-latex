@inproceedings{NIPS2017_3f5ee243,
    author      = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
    booktitle   = {Advances in Neural Information Processing Systems},
    editor      = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages       = {},
    publisher   = {Curran Associates, Inc.},
    title       = {Attention is All you Need},
    url         = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    volume      = {30},
    year        = {2017}
}

@inproceedings{image16x16words,
    title       = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author      = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle   = {International Conference on Learning Representations},
    year        = {2021},
    url         = {https://openreview.net/forum?id=YicbFdNTTy}
}

@InProceedings{image_transformer,
    title       = {Image Transformer},
    author      = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
    booktitle   = {Proceedings of the 35th International Conference on Machine Learning},
    pages       = {4055--4064},
    year        = {2018},
    editor      = {Dy, Jennifer and Krause, Andreas},
    volume      = {80},
    series      = {Proceedings of Machine Learning Research},
    month       = {10--15 Jul},
    publisher   = {PMLR},
    pdf         = {http://proceedings.mlr.press/v80/parmar18a/parmar18a.pdf},
    url         = {http://proceedings.mlr.press/v80/parmar18a.html}
}

@InProceedings{detrfacebookdetectiontransformers,
author="Carion, Nicolas
and Massa, Francisco
and Synnaeve, Gabriel
and Usunier, Nicolas
and Kirillov, Alexander
and Zagoruyko, Sergey",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="End-to-End Object Detection with Transformers",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="213--229",
isbn="978-3-030-58452-8"
}

@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@ARTICLE{2020arXiv200906732T,
       author = {{Tay}, Yi and {Dehghani}, Mostafa and {Bahri}, Dara and {Metzler}, Donald},
        title = "{Efficient Transformers: A Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
         year = 2020,
        month = sep,
          eid = {arXiv:2009.06732},
        pages = {arXiv:2009.06732},
archivePrefix = {arXiv},
       eprint = {2009.06732},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200906732T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{pmlr-v37-xuc15,
  title = 	 {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author = 	 {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2048--2057},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/xuc15.pdf},
  url = 	 {
http://proceedings.mlr.press/v37/xuc15.html
},
  abstract = 	 {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.}
}

@INPROCEEDINGS{7298685,
  author={Tianjun Xiao and Yichong Xu and Kuiyuan Yang and Jiaxing Zhang and Yuxin Peng and Zhang, Zheng},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={The application of two-level attention models in deep convolutional neural network for fine-grained image classification}, 
  year={2015},
  volume={},
  number={},
  pages={842-850},
  doi={10.1109/CVPR.2015.7298685}}
  
@INPROCEEDINGS{7410695,
author={Cao, Chunshui and Liu, Xianming and Yang, Yi and Yu, Yinan and Wang, Jiang and Wang, Zilei and Huang, Yongzhen and Wang, Liang and Huang, Chang and Xu, Wei and Ramanan, Deva and Huang, Thomas S.},
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks}, 
year={2015},
volume={},
number={},
pages={2956-2964},
doi={10.1109/ICCV.2015.338}}

@conference{neuralmachinetranslationalignandtranslate,
title = "Neural machine translation by jointly learning to align and translate",
abstract = "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
author = "Dzmitry Bahdanau and Cho, {Kyung Hyun} and Yoshua Bengio",
year = "2015",
month = jan,
day = "1",
language = "English (US)",
note = "3rd International Conference on Learning Representations, ICLR 2015 ; Conference date: 07-05-2015 Through 09-05-2015",
}

@inproceedings{NIPS1989_6c9882bb,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1990}
}

@inproceedings{NIPS2015_ae0eb3ee,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning both Weights and Connections for Efficient Neural Network},
 url = {https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 volume = {28},
 year = {2015}
}

@misc{vadera2020methods,
      title={Methods for Pruning Deep Neural Networks}, 
      author={Sunil Vadera and Salem Ameen},
      year={2020},
      eprint={2011.00241},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{2016arXiv160703250H,
       author = {{Hu}, Hengyuan and {Peng}, Rui and {Tai}, Yu-Wing and {Tang}, Chi-Keung},
        title = "{Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = 2016,
        month = jul,
          eid = {arXiv:1607.03250},
        pages = {arXiv:1607.03250},
archivePrefix = {arXiv},
       eprint = {1607.03250},
 primaryClass = {cs.NE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160703250H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{7303876,
  author={Polyak, Adam and Wolf, Lior},
  journal={IEEE Access}, 
  title={Channel-level acceleration of deep face representations}, 
  year={2015},
  volume={3},
  number={},
  pages={2163-2175},
  doi={10.1109/ACCESS.2015.2494536}}
  
  @inproceedings{NIPS1993_b056eb15,
 author = {Hassibi, Babak and Stork, David and Wolff, Gregory},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Surgeon: Extensions and performance comparisons},
 url = {https://proceedings.neurips.cc/paper/1993/file/b056eb1587586b71e2da9acfe4fbd19e-Paper.pdf},
 volume = {6},
 year = {1994}
}

@misc{molchanov2017pruning,
      title={Pruning Convolutional Neural Networks for Resource Efficient Inference}, 
      author={Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
      year={2017},
      eprint={1611.06440},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{pmlr-v97-wang19g, title = {{E}igen{D}amage: Structured Pruning in the {K}ronecker-Factored Eigenbasis}, author = {Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {6566--6575}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/wang19g/wang19g.pdf}, url = { http://proceedings.mlr.press/v97/wang19g.html }, abstract = {Reducing the test time resource requirements of a neural network while preserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce a novel network reparameterization based on the Kronecker-factored eigenbasis (KFE), and then apply Hessian-based structured pruning methods in this basis. As opposed to existing Hessian-based pruning algorithms which do pruning in parameter coordinates, our method works in the KFE where different weights are approximately independent, enabling accurate pruning and fast computation. We demonstrate empirically the effectiveness of the proposed method through extensive experiments. In particular, we highlight that the improvements are especially significant for more challenging datasets and networks. With negligible loss of accuracy, an iterative-pruning version gives a 10x reduction in model size and a 8x reduction in FLOPs on wide ResNet32.} }

@INPROCEEDINGS{8354187,
  author={Huang, Qiangui and Zhou, Kevin and You, Suya and Neumann, Ulrich},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Learning to Prune Filters in Convolutional Neural Networks}, 
  year={2018},
  volume={},
  number={},
  pages={709-718},
  doi={10.1109/WACV.2018.00083}}
  
  @misc{micikevicius2018mixed,
      title={Mixed Precision Training}, 
      author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
      year={2018},
      eprint={1710.03740},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{visiontransformersDPT,
      title={Vision Transformers for Dense Prediction}, 
      author={René Ranftl and Alexey Bochkovskiy and Vladlen Koltun},
      year={2021},
      eprint={2103.13413},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@techreport{ai_index_report_2021,
	address = {Human-Centered AI Institute, Stanford University, Stanford, CA},
	title = {The {AI} {Index} 2021 {Annual} {Report}},
	url = {https://aiindex.stanford.edu/report/},
	institution = {AI Index Steering Committee},
	author = {Zhang, Daniel and Mishra, Saurabh and Brynjolfsson, Erik and Etchemendy, John and Ganguli, Deep and Grosz, Barbara and Lyons, Terah and Manyika, James and Niebles, Juan Carlos and Sellitto, Michael and Shoham, Yoav and Clark, Jack and Perrault, Raymond},
	month = mar,
	year = {2021},
}

@inproceedings{qiu-etal-2020-blockwise,
    title = "Blockwise Self-Attention for Long Document Understanding",
    author = "Qiu, Jiezhong  and
      Ma, Hao  and
      Levy, Omer  and
      Yih, Wen-tau  and
      Wang, Sinong  and
      Tang, Jie",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.232",
    doi = "10.18653/v1/2020.findings-emnlp.232",
    pages = "2555--2565",
    abstract = "We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1{\%} less memory and 12.0-25.1{\%} less time to learn the model. During testing, BlockBERT saves 27.8{\%} inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.",
}

@ARTICLE{sparse-transformers,
       author = {{Child}, Rewon and {Gray}, Scott and {Radford}, Alec and {Sutskever}, Ilya},
        title = "{Generating Long Sequences with Sparse Transformers}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2019,
        month = apr,
          eid = {arXiv:1904.10509},
        pages = {arXiv:1904.10509},
archivePrefix = {arXiv},
       eprint = {1904.10509},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190410509C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{beltagy2020longformer,
  abstract = {     Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. },
  added-at = {2020-10-28T11:32:53.000+0100},
  archiveprefix = {arXiv},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  biburl = {https://www.bibsonomy.org/bibtex/22435246630c361e01f31009339c70f41/ghagerer},
  eprint = {2004.05150},
  interhash = {9197cfbaf90615f175718342fe883457},
  intrahash = {2435246630c361e01f31009339c70f41},
  keywords = {bert pre-trained transfer-learning transformer},
  primaryclass = {cs.CL},
  timestamp = {2020-10-28T11:32:53.000+0100},
  title = {Longformer: The Long-Document Transformer},
  url = {https://arxiv.org/abs/2004.05150},
  year = 2020
}

@inproceedings{
j.2018generating,
title={Generating Wikipedia by Summarizing Long Sequences},
author={Peter J. Liu* and Mohammad Saleh* and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hyg0vbWC-},
}

@misc{ho2019axial,
      title={Axial Attention in Multidimensional Transformers}, 
      author={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},
      year={2019},
      eprint={1912.12180},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2020linformer,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
Kitaev2020Reformer:,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@inproceedings{localattention,
 author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Stand-Alone Self-Attention in Vision Models},
 url = {https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{routingtransformer,
    author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
    title = "{Efficient Content-Based Sparse Attention with Routing Transformers}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {53-68},
    year = {2021},
    month = {02},
    abstract = "{Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00353},
    url = {https://doi.org/10.1162/tacl\_a\_00353},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00353/1891548/tacl\_a\_00353.pdf},
}

@inproceedings{set-transformer,
  year = {2019},
  edition = {},
  number = {},
  journal = {},
  pages = {3744-3753},
  publisher = {Proceedings of Machine Learning Research},
  school = {},
  title = {Set transformer: A framework for attention-based permutation-invariant neural networks},
  volume = {97},
  author = {Lee, J and Lee, Y and Kim, J and Kosiorek, AR and Choi, S and Teh, YW},
  editor = {},
  organizer = {36th International Conference on Machine Learning (ICML 2019)},
  series = {}
}

@inproceedings{ainslie-etal-2020-etc,
    title = "{ETC}: Encoding Long and Structured Inputs in Transformers",
    author = "Ainslie, Joshua  and
      Ontanon, Santiago  and
      Alberti, Chris  and
      Cvicek, Vaclav  and
      Fisher, Zachary  and
      Pham, Philip  and
      Ravula, Anirudh  and
      Sanghai, Sumit  and
      Wang, Qifan  and
      Yang, Li",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.19",
    doi = "10.18653/v1/2020.emnlp-main.19",
    pages = "268--284",
    abstract = "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, {``}Extended Transformer Construction{''} (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a {``}Contrastive Predictive Coding{''} (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",
}


@Article{faststereodepth,
AUTHOR = {Aguilera, Cristhian A. and Aguilera, Cristhian and Navarro, Cristóbal A. and Sappa, Angel D.},
TITLE = {Fast CNN Stereo Depth Estimation through Embedded GPU Devices},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3249},
URL = {https://www.mdpi.com/1424-8220/20/11/3249},
ISSN = {1424-8220},
ABSTRACT = {Current CNN-based stereo depth estimation models can barely run under real-time constraints on embedded graphic processing unit (GPU) devices. Moreover, state-of-the-art evaluations usually do not consider model optimization techniques, being that it is unknown what is the current potential on embedded GPU devices. In this work, we evaluate two state-of-the-art models on three different embedded GPU devices, with and without optimization methods, presenting performance results that illustrate the actual capabilities of embedded GPU devices for stereo depth estimation. More importantly, based on our evaluation, we propose the use of a U-Net like architecture for postprocessing the cost-volume, instead of a typical sequence of 3D convolutions, drastically augmenting the runtime speed of current models. In our experiments, we achieve real-time inference speed, in the range of 5&ndash;32 ms, for 1216 &times; 368 input stereo images on the Jetson TX2, Jetson Xavier, and Jetson Nano embedded devices.},
DOI = {10.3390/s20113249}
}

@inproceedings{pydnet18,
  title     = {Towards real-time unsupervised monocular depth estimation on CPU},
  author    = {Poggi, Matteo and
               Aleotti, Filippo and
               Tosi, Fabio and
               Mattoccia, Stefano},
  booktitle = {IEEE/JRS Conference on Intelligent Robots and Systems (IROS)},
  year = {2018}
}

@article{mininet,
author = {Liu, Jun and Li, Qing and Cao, Rui and Tang, Wenming and Qiu, Guoping},
year = {2020},
month = {08},
pages = {255-267},
title = {MiniNet: An extremely lightweight convolutional neural network for real-time unsupervised monocular depth estimation},
volume = {166},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
doi = {10.1016/j.isprsjprs.2020.06.004}
}

@inproceedings{icra_2019_fastdepth,
	author      = {{Wofk, Diana and Ma, Fangchang and Yang, Tien-Ju and Karaman, Sertac and Sze, Vivienne}},
	title       = {{FastDepth: Fast Monocular Depth Estimation on Embedded Systems}},
	booktitle   = {{IEEE International Conference on Robotics and Automation (ICRA)}},
	year        = {{2019}}
}


@InProceedings{eccv_2018_yang_netadapt,
	author = {Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
	title = {NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications},
	booktitle = {The European Conference on Computer Vision (ECCV)},
	month = {September},
	year = {2018}
}

@misc{wang2020mobiledepth,
      title={MobileDepth: Efficient Monocular Depth Prediction on Mobile Devices}, 
      author={Yekai Wang},
      year={2020},
      eprint={2011.10189},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{realtime-monocular-style-transfer,
  author={Atapour-Abarghouei, Amir and Breckon, Toby P.},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Real-Time Monocular Depth Estimation Using Synthetic Data with Domain Adaptation via Image Style Transfer}, 
  year={2018},
  volume={},
  number={},
  pages={2800-2810},
  doi={10.1109/CVPR.2018.00296}}
  
  @inproceedings{nyudepthv2,
  author    = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},
  title     = {Indoor Segmentation and Support Inference from RGBD Images},
  booktitle = {ECCV},
  year      = {2012}
}

@INPROCEEDINGS{depth-estimation-metrics,  author={Cadena, Cesar and Latif, Yasir and Reid, Ian D.},  booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},   title={Measuring the performance of single image depth estimation methods},   year={2016},  volume={},  number={},  pages={4150-4157},  doi={10.1109/IROS.2016.7759611}}

@article{mobilenets,
author = {Howard, Andrew and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
year = {2017},
month = {04},
pages = {},
title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}
}

@INPROCEEDINGS {regnety,
author = {I. Radosavovic and R. Kosaraju and R. Girshick and K. He and P. Dollar},
booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Designing Network Design Spaces},
year = {2020},
volume = {},
issn = {},
pages = {10425-10433},
keywords = {computational modeling;manuals;tools;sociology;statistics;training;visualization},
doi = {10.1109/CVPR42600.2020.01044},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.01044},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@InProceedings{kernel-transformer, title = {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention}, author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {5156--5165}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf}, url = { http://proceedings.mlr.press/v119/katharopoulos20a.html }, abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our \emph{Linear Transformers} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.} }

@INPROCEEDINGS{importancestereo,  author={Smolyanskiy, Nikolai and Kamenev, Alexey and Birchfield, Stan},  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},   title={On the Importance of Stereo for Accurate Depth Estimation: An Efficient Semi-Supervised Deep Neural Network Approach},   year={2018},  volume={},  number={},  pages={1120-11208},  doi={10.1109/CVPRW.2018.00147}}

@InProceedings{lidarcompletion,
author = {Xu, Yan and Zhu, Xinge and Shi, Jianping and Zhang, Guofeng and Bao, Hujun and Li, Hongsheng},
title = {Depth Completion From Sparse LiDAR Data With Depth-Normal Constraints},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{sparse-to-dense,
	title={Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera},
	author={Ma, Fangchang and Cavalheiro, Guilherme Venturelli and Karaman, Sertac},
	booktitle={ICRA},
	year={2019}
}

@INPROCEEDINGS{domain-adaptation,
  author={Zhao, Shanshan and Fu, Huan and Gong, Mingming and Tao, Dacheng},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={9780-9790},
  doi={10.1109/CVPR.2019.01002}}
  
  @INPROCEEDINGS{monocularstereosynthesis,
  author={Tosi, Fabio and Aleotti, Filippo and Poggi, Matteo and Mattoccia, Stefano},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Monocular Depth Estimation Infusing Traditional Stereo Knowledge}, 
  year={2019},
  volume={},
  number={},
  pages={9791-9801},
  doi={10.1109/CVPR.2019.01003}}
  
  @inproceedings{deep3d,
  title={Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks},
  author={Junyuan Xie and Ross B. Girshick and Ali Farhadi},
  booktitle={ECCV},
  year={2016}
}

@INPROCEEDINGS{single-view-stereo-matching,
  author={Luo, Yue and Ren, Jimmy and Lin, Mude and Pang, Jiahao and Sun, Wenxiu and Li, Hongsheng and Lin, Liang},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Single View Stereo Matching}, 
  year={2018},
  volume={},
  number={},
  pages={155-163},
  doi={10.1109/CVPR.2018.00024}}
  
  @ARTICLE{lidarlossfunction,
  author={He, Li and Chen, Chuangbin and Zhang, Tao and Zhu, Haife and Wan, Shaohua},
  journal={IEEE Access}, 
  title={Wearable Depth Camera: Monocular Depth Estimation via Sparse Optimization Under Weak Supervision}, 
  year={2018},
  volume={6},
  number={},
  pages={41337-41345},
  doi={10.1109/ACCESS.2018.2857703}}
  
  @article{Zhao_2020,
   title={Monocular depth estimation based on deep learning: An overview},
   volume={63},
   ISSN={1869-1900},
   url={http://dx.doi.org/10.1007/s11431-020-1582-8},
   DOI={10.1007/s11431-020-1582-8},
   number={9},
   journal={Science China Technological Sciences},
   publisher={Springer Science and Business Media LLC},
   author={Zhao, ChaoQiang and Sun, QiYu and Zhang, ChongZhen and Tang, Yang and Qian, Feng},
   year={2020},
   month={Jun},
   pages={1612–1627}
}

@inproceedings{zhou2017unsupervised,
    Author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
    Title = {Unsupervised Learning of Depth and Ego-Motion from Video},
    Booktitle = {CVPR},
    Year = {2017}
}

@inproceedings{geo_mask_egomotion,
 author = {Bian, Jiawang and Li, Zhichao and Wang, Naiyan and Zhan, Huangying and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video},
 url = {https://proceedings.neurips.cc/paper/2019/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{vijayanarasimhan2017sfmnet,
      title={SfM-Net: Learning of Structure and Motion from Video}, 
      author={Sudheendra Vijayanarasimhan and Susanna Ricco and Cordelia Schmid and Rahul Sukthankar and Katerina Fragkiadaki},
      year={2017},
      eprint={1704.07804},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{geonet,
  author={Yin, Zhichao and Shi, Jianping},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose}, 
  year={2018},
  volume={},
  number={},
  pages={1983-1992},
  doi={10.1109/CVPR.2018.00212}}
  
  @INPROCEEDINGS{visualodometryunsupervised,
  author={Wang, Chaoyang and Buenaposada, José Miguel and Zhu, Rui and Lucey, Simon},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Learning Depth from Monocular Videos Using Direct Methods}, 
  year={2018},
  volume={},
  number={},
  pages={2022-2030},
  doi={10.1109/CVPR.2018.00216}}
  
  @article{hu2020PENet,
	title={Towards Precise and Efficient Image Guided Depth Completion},
	author={Hu, Mu and Wang, Shuling and Li, Bin and Ning, Shiyu and Fan, Li and Gong, Xiaojin},
	booktitle={ICRA},
	year={2021}
}

@article{zhql2021ResT,
  title={ResT: An Efficient Transformer for Visual Recognition},
  author={Zhang, Qinglong and Yang, Yubin},
  journal={arXiv preprint arXiv:2105.13677v2},
  year={2021}
}

@misc{bhat2020adabins,
      title={AdaBins: Depth Estimation using Adaptive Bins}, 
      author={Shariq Farooq Bhat and Ibraheem Alhashim and Peter Wonka},
      year={2020},
      eprint={2011.14141},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{gan,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{chen2021transunet,
  title={TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation},
  author={Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L. and Zhou, Yuyin},
  journal={arXiv preprint arXiv:2102.04306},
  year={2021}
}

@article{liu2021Swin,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={arXiv preprint arXiv:2103.14030},
  year={2021}
}

@inproceedings{eigen-multi-scale,
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
title = {Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2366–2374},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@INPROCEEDINGS{surfacenormals,
  author={Eigen, David and Fergus, Rob},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture}, 
  year={2015},
  volume={},
  number={},
  pages={2650-2658},
  doi={10.1109/ICCV.2015.304}}
  
  @INPROCEEDINGS{gan-depth,
  author={Jung, Hyungjoo and Kim, Youngjung and Min, Dongbo and Oh, Changjae and Sohn, Kwanghoon},
  booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, 
  title={Depth prediction from a single image with conditional adversarial networks}, 
  year={2017},
  volume={},
  number={},
  pages={1717-1721},
  doi={10.1109/ICIP.2017.8296575}}
  
  @misc{zwald2012berhu,
    title={The BerHu penalty and the grouped effect},
    author={Laurent Zwald and Sophie Lambert-Lacroix},
    year={2012},
    eprint={1207.6868},
    archivePrefix={arXiv},
    primaryClass={math.ST}
}

@INPROCEEDINGS{monodepth,
  author={Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Digging Into Self-Supervised Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={3827-3837},
  doi={10.1109/ICCV.2019.00393}}
  
@ARTICLE{midas-intel,
  author={Ranftl, Rene and Lasinger, Katrin and Hafner, David and Schindler, Konrad and Koltun, Vladlen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2020.3019967}}
  
@InProceedings{evaluation-cnn-depth-estimation,
author = {Koch, Tobias and Liebel, Lukas and Fraundorfer, Friedrich and Korner, Marco},
title = {Evaluation of CNN-based Single-Image Depth Estimation Methods},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
month = {September},
year = {2018}
}

@inproceedings{DORN,
  TITLE = {{Deep Ordinal Regression Network for Monocular Depth Estimation}},
  AUTHOR = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
  BOOKTITLE = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  YEAR = {2018}
}

@article{bts,
  title={From big to small: Multi-scale local planar guidance for monocular depth estimation},
  author={Lee, Jin Han and Han, Myung-Kyu and Ko, Dong Wook and Suh, Il Hong},
  journal={arXiv preprint arXiv:1907.10326},
  year={2019}
}

@ARTICLE{KITTI-dataset,
  author = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun},
  title = {Vision meets Robotics: The KITTI Dataset},
  journal = {International Journal of Robotics Research (IJRR)},
  year = {2013}
}

@INPROCEEDINGS{KITTI-benchmarks,
  author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2012}
}

@INPROCEEDINGS{KITTI-road-benchmark,
  author = {Jannik Fritsch and Tobias Kuehnl and Andreas Geiger},
  title = {A New Performance Measure and Evaluation Benchmark for Road Detection Algorithms},
  booktitle = {International Conference on Intelligent Transportation Systems (ITSC)},
  year = {2013}
}

@INPROCEEDINGS{KITTI-sceneflow-benchmark,
  author = {Moritz Menze and Andreas Geiger},
  title = {Object Scene Flow for Autonomous Vehicles},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2015}
}

@BOOK{Kalloniatis2005-pc,
  title  = {Webvision: The Organization of the Retina and Visual System},
  author = {Kalloniatis, M and Luu, C},
  editor = {Kolb, H},
  year   = {2005}
}

@inbook{hartley_zisserman_2004, 
	place={Cambridge}, 
	edition={2}, 
	title={Two-View Geometry}, 
	DOI={10.1017/CBO9780511811685.013}, 
	booktitle={Multiple View Geometry in Computer Vision}, 
	publisher={Cambridge University Press}, 
	author={Hartley, Richard and Zisserman, Andrew}, 
	year={2004}, 
	pages={237–238}
}

@article{wang2019apolloscape,
  title={The apolloscape open dataset for autonomous driving and its application},
  author={Wang, Peng and Huang, Xinyu and Cheng, Xinjing and Zhou, Dingfu and Geng, Qichuan and Yang, Ruigang},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2019},
  publisher={IEEE}
}

@InProceedings{Xian_2020_CVPR,
author = {Xian, Ke and Zhang, Jianming and Wang, Oliver and Mai, Long and Lin, Zhe and Cao, Zhiguo},
title = {Structure-Guided Ranking Loss for Single Image Depth Prediction},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{tartanair2020iros,
  title =   {TartanAir: A Dataset to Push the Limits of Visual SLAM},
  author =  {Wang, Wenshan and Zhu, Delong and Wang, Xiangwei and Hu, Yaoyu and Qiu, Yuheng and Wang, Chen and Hu, Yafei and Kapoor, Ashish and Scherer, Sebastian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year =    {2020}
}

@INPROCEEDINGS {IRS,
author = {Q. Wang and S. Zheng and Q. Yan and F. Deng and K. Zhao and X. Chu},
booktitle = {2021 IEEE International Conference on Multimedia and Expo (ICME)},
title = {IRS: A Large Naturalistic Indoor Robotics Stereo Dataset to Train Deep Models for Disparity and Surface Normal Estimation},
year = {2021},
volume = {},
issn = {},
pages = {1-6},
keywords = {training;surface reconstruction;multimedia systems;refining;estimation;rendering (computer graphics);stereo vision},
doi = {10.1109/ICME51207.2021.9428423},
url = {https://doi.ieeecomputersociety.org/10.1109/ICME51207.2021.9428423},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@misc{wang2019web,
    title={Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes},
    author={Chaoyang Wang and Simon Lucey and Federico Perazzi and Oliver Wang},
    year={2019},
    eprint={1904.11112},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@INPROCEEDINGS{blendedMVS,
  author={Yao, Yao and Luo, Zixin and Li, Shiwei and Zhang, Jingyang and Ren, Yufan and Zhou, Lei and Fang, Tian and Quan, Long},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={BlendedMVS: A Large-Scale Dataset for Generalized Multi-View Stereo Networks}, 
  year={2020},
  volume={},
  number={},
  pages={1787-1796},
  doi={10.1109/CVPR42600.2020.00186}
  }
  
@inProceedings{MegaDepthLi18,
  title={MegaDepth: Learning Single-View Depth Prediction from Internet Photos},
  author={Zhengqi Li and Noah Snavely},
  booktitle={Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}

@inproceedings{Xian_2018_CVPR,
    title={Monocular Relative Depth Perception with Web Stereo Data Supervision},
    author= {Xian, Ke and Shen, Chunhua and Cao, Zhiguo and Lu, Hao and Xiao, Yang and Li, Ruibo and Luo, Zhenbo},
    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month={June},
    year={2018}
}

@ARTICLE{DIML,
  author={Kim, Youngjung and Jung, Hyungjoo and Min, Dongbo and Sohn, Kwanghoon},
  journal={IEEE Transactions on Image Processing}, 
  title={Deep Monocular Depth Estimation via Integration of Global and Local Predictions}, 
  year={2018},
  volume={27},
  number={8},
  pages={4131-4144},
  doi={10.1109/TIP.2018.2836318}
  }
  
 @inproceedings{DIW,
 author = {Chen, Weifeng and Fu, Zhao and Yang, Dawei and Deng, Jia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Single-Image Depth Perception in the Wild},
 url = {https://proceedings.neurips.cc/paper/2016/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{schoeps2017cvpr,
  author = {Thomas Sch\"ops and Johannes L. Sch\"onberger and Silvano Galliani and Torsten Sattler and Konrad Schindler and Marc Pollefeys and Andreas Geiger},
  title = {A Multi-View Stereo Benchmark with High-Resolution Images and Multi-Camera Videos},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2017}
}

@inproceedings{Butler:ECCV:2012,
title = {A naturalistic open source movie for optical flow evaluation},
author = {Butler, D. J. and Wulff, J. and Stanley, G. B. and Black, M. J.},
booktitle = {European Conf. on Computer Vision (ECCV)},
editor = {{A. Fitzgibbon et al. (Eds.)}},
publisher = {Springer-Verlag},
series = {Part IV, LNCS 7577},
month = oct,
pages = {611--625},
year = {2012}
}

@InProceedings{ sturm12iros,
	author = {J. Sturm and N. Engelhard and F. Endres and W. Burgard and D. Cremers},
	title = "A Benchmark for the Evaluation of RGB-D SLAM Systems",
	booktitle = "Proc. of the International Conference on Intelligent Robot Systems (IROS)",
	year = "2012",
	month= "Oct.",
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@book{wordnet,
  abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
  added-at = {2017-11-01T11:46:20.000+0100},
  address = {Cambridge, MA},
  biburl = {https://www.bibsonomy.org/bibtex/28472b4f9d7f2bfc4a97ffd4a023facc6/flint63},
  editor = {Fellbaum, Christiane},
  file = {eBook:1900-99/Fellbaum1998.pdf:PDF;MIT Press Product Page:http\://mitpress.mit.edu/books/wordnet:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/026206197X/:URL},
  groups = {public},
  interhash = {42daa1681607dd1d3f3234c605d84ec3},
  intrahash = {8472b4f9d7f2bfc4a97ffd4a023facc6},
  isbn = {978-0-262-06197-1},
  keywords = {01821 101 mitpress book shelf ai language processing ontology lexicon},
  publisher = {MIT Press},
  series = {Language, Speech, and Communication},
  timestamp = {2018-04-16T11:51:58.000+0200},
  title = {WordNet: An Electronic Lexical Database},
  username = {flint63},
  year = 1998
}

 @misc{harris2012, 
 title={How to optimize data transfers in CUDA C/C++}, 
 url={https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/}, 
 journal={NVIDIA Developer Blog}, 
 author={Harris, Mark}, 
 year={2012}, 
 month={Dec}
 } 

@article{universal-approximators,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{backprop,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
  doi = {10.1038/323533a0},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
  journal = {Nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{Learning Representations by Back-propagating Errors}},
  url = {http://www.nature.com/articles/323533a0},
  volume = 323,
  year = 1986
}

@article{model-evaluation,
  author    = {Sebastian Raschka},
  title     = {Model Evaluation, Model Selection, and Algorithm Selection in Machine
               Learning},
  journal   = {CoRR},
  volume    = {abs/1811.12808},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12808},
  eprinttype = {arXiv},
  eprint    = {1811.12808},
  timestamp = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-12808.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@article{gelu,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@article{adagrad,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121-2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}

@inproceedings{adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)},
  crossref={conf/iclr/2015}
}

@inproceedings{adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@conference{on-large-batch,
title = "On large-batch training for deep learning: Generalization gap and sharp minima",
abstract = "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",
author = "Keskar, {Nitish Shirish} and Jorge Nocedal and Tang, {Ping Tak Peter} and Dheevatsa Mudigere and Mikhail Smelyanskiy",
year = "2017",
language = "English (US)",
note = "5th International Conference on Learning Representations, ICLR 2017 ; Conference date: 24-04-2017 Through 26-04-2017",
}

@Inbook{LeCun1999,
author="LeCun, Yann
and Haffner, Patrick
and Bottou, L{\'e}on
and Bengio, Yoshua",
title="Object Recognition with Gradient-Based Learning",
bookTitle="Shape, Contour and Grouping in Computer Vision",
year="1999",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="319--345",
abstract="Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations.",
isbn="978-3-540-46805-9",
doi="10.1007/3-540-46805-6_19",
url="https://doi.org/10.1007/3-540-46805-6_19"
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}
  
  
@InProceedings{efficientnet,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/tan19a.html},
  abstract = 	 {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}

@INPROCEEDINGS{mnasnet,
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={MnasNet: Platform-Aware Neural Architecture Search for Mobile}, 
  year={2019},
  volume={},
  number={},
  pages={2815-2823},
  doi={10.1109/CVPR.2019.00293}}

@INPROCEEDINGS{mobilenetv2,
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={MobileNetV2: Inverted Residuals and Linear Bottlenecks}, 
  year={2018},
  volume={},
  number={},
  pages={4510-4520},
  doi={10.1109/CVPR.2018.00474}}

@INPROCEEDINGS{googlelenet,
  author={Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Going deeper with convolutions}, 
  year={2015},
  volume={},
  number={},
  pages={1-9},
  doi={10.1109/CVPR.2015.7298594}}

@INPROCEEDINGS{senet,
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Squeeze-and-Excitation Networks}, 
  year={2018},
  volume={},
  number={},
  pages={7132-7141},
  doi={10.1109/CVPR.2018.00745}}
  
@InProceedings{refinenet,
author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
title = {RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{are16headsbetterthan1,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@Article{numpy,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@Article{matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}

@article{seaborn,
    doi = {10.21105/joss.03021},
    url = {https://doi.org/10.21105/joss.03021},
    year = {2021},
    publisher = {The Open Journal},
    volume = {6},
    number = {60},
    pages = {3021},
    author = {Michael L. Waskom},
    title = {seaborn: statistical data visualization},
    journal = {Journal of Open Source Software}
 }
 
 @incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@misc{pwperformer,
  author = {Phil Wang},
  title = {Performer Pytorch},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/lucidrains/performer-pytorch}},
}

@misc{wandb,
	title = {Experiment Tracking with Weights and Biases},
	year = {2020},
	note = {Software available from wandb.com},
	url={https://www.wandb.com/},
	author = {Biewald, Lukas},
}

@misc{fvcore,
  author = {FAIR},
  title = {fvcore},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/facebookresearch/fvcore}},
}

@article{glpdepth,
  title={Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth},
  author={Kim, Doyeon and Ga, Woonghyun and Ahn, Pyungwhan and Joo, Donggyu and Chun, Sehwan and Kim, Junmo},
  journal={arXiv preprint arXiv:2201.07436},
  year={2022}
}

@inproceedings{performers,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@InProceedings{unet,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}

@misc{yolov3,
  doi = {10.48550/ARXIV.1804.02767},
  
  url = {https://arxiv.org/abs/1804.02767},
  
  author = {Redmon, Joseph and Farhadi, Ali},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {YOLOv3: An Incremental Improvement},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{alexnet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}
