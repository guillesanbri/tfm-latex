\section{Conclusiones y Lineas Futuras}
En este Trabajo Fin de Máster, se han estudiado: el campo de la estimación de profundidad, haciendo especial hincapié en el uso de imágenes monoculares y las técnicas de aprendizaje automático supervisado propuestas para este problema; las arquitecturas basadas en \textit{transformers}; y distintas técnicas dirigidas a acelerar el entrenamiento y la inferencia, tanto generales como dirigidas a arquitecturas concretas. Sobre esta base de conocimiento, se han definido una serie de modificaciones en una de las arquitecturas del estado del arte actual en la estimación de profundidad a partir de imágenes monoculares. Estas modificaciones se han implementado para entrenar los modelos resultantes (con un equipo propio y recursos en la nube) en un conjunto de datos enfocado a la conducción autónoma, KITTI. Una vez los modelos han sido entrenados, se ha llevado a cabo un estudio del grado de influencia de cada una de las modificaciones planteadas en distintas métricas relacionadas con la calidad de los resultados, la velocidad de inferencia, o el tamaño de los modelos modificados. 

En base a este estudio de los resultados, se ha seleccionado una configuración que se considera valiosa al tener un equilibrio entre aumento de la velocidad de inferencia, reducción del tamaño del modelo y pérdida de la calidad de los resultados. Junto a esta memoria, se publica el código necesario para entrenar y usar estos modelos, así como sus parámetros tras el entrenamiento, con el objetivo de que sea posible continuar ampliando este trabajo y sus resultados.

En cuanto a las líneas por las que continuar con este proyecto, queda aún pendiente adaptar el modelo de DPT y las modificaciones planteadas para funcionar en \textit{hardware} empotrado, para lo que sería necesario solucionar las limitaciones encontradas relacionadas con la cuantificación (es decir, desarrollar o ampliar los \textit{frameworks} necesarios para cuantificar estos modelos). Por otro lado, sería también interesante entrenar las modificaciones sin reducir el tamaño de entrada, o recopilar los \textit{datasets} que componen MIX6 para entrenar parte de los modelos propuestos, llevando a cabo un estudio exhaustivo de la influencia del preentrenamiento empleando un \textit{dataset} con imágenes de profundidad en comparación con un \textit{dataset} más general como es ImageNet. Además, visto el sobreajuste de los modelos cuando se emplea EfficientNet como \textit{backbone} convolucional, añadir una fuerte regularización al entrenamiento de este grupo de modelos y estudiar sus resultados sería beneficioso. Por último, y con un carácter más aplicado, se podría continuar con este Trabajo Fin de Máster implementando los modelos presentados como parte de una aplicación completa (SLAM monocular, conducción autónoma, reconstrucción 3D, realidad aumentada, etc.) incluso llegando a aprovechar los bloques de atención eficientes con imágenes mucho mayores (por ejemplo, con imágenes satelitales o fotografías de muy alta resolución).

\pagebreak

\todo[inline]{Esto es la demostración de la invariancia a la escala de la función de pérdida y de la métrica SILog, dedicarle un anexo me parece excesivo pero sí que me parece interesante}

\begin{align*}
    L(\hat{d}, d) = \frac{1}{n}\sum_{p} (\ln{\hat{d_p}} - \ln{d_p})^2 - \frac{1}{n^2} \left( \sum_{p} (\ln{\hat{d_p}} - \ln{d_p}) \right)^2 \\
    = \frac{1}{n}\sum_{p} (\ln{\alpha \hat{d_p}} - \ln{d_p})^2 - \frac{1}{n^2} \left( \sum_{p} (\ln{\alpha \hat{d_p}} - \ln{d_p}) \right)^2 \\
    = \frac{1}{n}\sum_{p} (\ln{\alpha} + \ln{\frac{\hat{d_p}}{d_p}})^2 - \frac{1}{n^2} \left( \sum_{p} (\ln{\alpha} + \ln{\frac{\hat{d_p}}{d_p}}) \right)^2 \\
    = \frac{1}{n}\sum_{p} (\beta + \ln{\frac{\hat{d_p}}{d_p}})^2 - \frac{1}{n^2} \left( \sum_{p} (\beta + \ln{\frac{\hat{d_p}}{d_p}}) \right)^2 \\
    = \frac{1}{n}\sum_{p} (\beta^2 + \ln{\frac{\hat{d_p}}{d_p}}^2 + 2 \beta ln{\frac{\hat{d_p}}{d_p}}) - \frac{1}{n^2} \left( \beta n + \sum_{p} (\ln{\frac{\hat{d_p}}{d_p}}) \right)^2 \\
    = \frac{1}{n} (\beta^2 n + \sum_{p}(\ln{\frac{\hat{d_p}}{d_p}})^2 + \sum_{p}(2 \beta ln{\frac{\hat{d_p}}{d_p}})) - \frac{1}{n^2} \left( \beta^2 n^2 + (\sum_{p} (\ln{\frac{\hat{d_p}}{d_p}}))^2 + 2 \beta n \sum_{p} (\ln{\frac{\hat{d_p}}{d_p}}) \right) \\
    = \beta^2 + \frac{\sum_{p}(\ln{\frac{\hat{d_p}}{d_p}})^2}{n} + \frac{\sum_{p}(2 \beta ln{\frac{\hat{d_p}}{d_p}})}{n} - \left( \beta^2 + \frac{(\sum_{p} (\ln{\frac{\hat{d_p}}{d_p}}))^2}{n^2} + \frac{2 \beta n \sum_{p} (\ln{\frac{\hat{d_p}}{d_p}})}{n^2} \right) \\
    = \beta^2 - \beta^2 + \frac{\sum_{p}(2 \beta ln{\frac{\hat{d_p}}{d_p}})}{n} - \frac{2 \beta \sum_{p} (\ln{\frac{\hat{d_p}}{d_p}})}{n} + \frac{\sum_{p}(\ln{\frac{\hat{d_p}}{d_p}})^2}{n} - \frac{(\sum_{p} (\ln{\frac{\hat{d_p}}{d_p}}))^2}{n^2} \\
    = \frac{\sum_{p}(\ln{\frac{\hat{d_p}}{d_p}})^2}{n} - \frac{(\sum_{p} (\ln{\frac{\hat{d_p}}{d_p}}))^2}{n^2} \\
    = \frac{1}{n}\sum_{p} (\ln{\hat{d_p}} - \ln{d_p})^2 - \frac{1}{n^2} \left( \sum_{p} (\ln{\hat{d_p}} - \ln{d_p}) \right)^2 \\
\end{align*}