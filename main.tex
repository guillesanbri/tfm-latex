%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Trabajo de fin de máster v1.0
% Guillermo Sánchez Brizuela
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Qué tipo de documento estamos por comenzar:
\documentclass[a4paper, 11pt]{article}
% Esto es para que el LaTeX sepa que el texto está en español:
\usepackage[spanish,es-tabla]{babel}
\selectlanguage{spanish}
% Esto es para poder escribir acentos directamente:
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% Para texto tachado
\usepackage[normalem]{ulem}
% Imágenes en el lateral
\usepackage{wrapfig}
% Tablas
\usepackage{booktabs}
% Anexos
\usepackage{appendix}
%Tablas
\usepackage{float}
\usepackage{tabularx}
%\usepackage{subcaption}
% Leyendas
\usepackage{caption}
\captionsetup{justification=centering}
% Bibliografia en la tabla de contenidos
\usepackage[nottoc,notlot,notlof]{tocbibind}
% Tablas
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}

%\setlength{\parskip}{\baselineskip}
\edef\restoreparindent{\parindent=\the\parindent\relax}
\usepackage{parskip}
\restoreparindent

% Definición de un nuevo comando
\newcommand{\textbfit}[1]{\textbf{\textit{#1}}}



%% Asigna un tamaño a la hoja y los márgenes
\usepackage[a4paper,top=3cm,bottom=2cm,left=2.5cm,right=2.5cm,marginparwidth=2cm]{geometry}

%% Paquetes de la AMS
\usepackage{amsmath, amsthm, amsfonts}
%% Para añadir archivos con extensión pdf, jpg, png or tif
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
%% Simbolo de grado
\usepackage{gensymb}
%% Imágenes en vertical
\usepackage{subfig}
\usepackage{graphicx}

\usepackage{nomencl}
\makenomenclature
\nomenclature{Estática}{(Imagen) Capturada o generada de forma individual y no como parte de un vídeo.}
\nomenclature{Vídeo}{Sucesión de imágenes continua que aporta ilusión de movimiento.}
\nomenclature{Online}{(Procesamiento) Que ocurre a medida que llega nueva información.}
\nomenclature{Offline}{(Procesamiento) Que trabaja sobre información previamente recogida.}
\def\nomname{Terminología}

%% Primero escribimos el título
\title{Borrador TFM/Memoria de prácticas: Estimación de Profundidad Monocular Online con Transformers Eficientes}
\author{Guillermo Sánchez Brizuela\\
  \small Universidad de Valladolid\\
  \small Valladolid, España
  \date{}
}

%% Después del "preámbulo", podemos empezar el documento

\begin{document}

\begin{titlepage}
\centering
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.35]{imagenes/gsi.png}
    \vspace{0.5cm}
\end{figure}%
	{\LARGE Memoria del trabajo realizado durante la estancia en el Grupo de Sistemas Inteligentes\par}
	\vspace{3cm}
	{\huge\bfseries Estimación de Profundidad Monocular Online con Transformers Eficientes\par}
	\vspace{0.5cm}
    {\LARGE Contexto y estado del arte\par}
    \vspace{3cm}
	{\Large\itshape Guillermo Sánchez Brizuela\par}
	\vspace{0.5cm}
	{\Large Universidad de Valladolid, curso 2020-2021\par}
	\vfill
\end{titlepage}

%% Imprimimos la nomenclatura
% \todo[inline]{Repasar las definiciones.}
% \printnomenclature[3cm]

%% Tabla de contenidos
{
    \setcounter{tocdepth}{4}
    \setcounter{secnumdepth}{4}
    \hypersetup{linkcolor=black}
    \tableofcontents
}

\newpage

%% Hay que decirle que incluya el título en el documento
% \maketitle

%% Aquí podemos añadir un resumen del trabajo (o del artículo en su caso) 
% \begin{abstract}
% Lorem ipsum.
% \end{abstract}

% \todo[inline]{Hacer portada y buscar normas del tfm. (TFM)}
%% Iniciamos "secciones" que servirán como subtítulos
\include{Introduccion}
\include{Marco-teorico-sota}
\include{Material-y-metodos}
\include{Modificaciones-y-desarrollo}
\include{Resultados}
\include{Discusion}
\include{Conclusiones-lineas-futuras}

% \bibliographystyle{abbrv}
\bibliographystyle{ieeetr}
\clearpage
\bibliography{referencias}
% \todo[inline]{Ver orden de referencias.}
% \todo[inline]{Ver si se pueden citar los de arXiv.}
% \todo[inline]{Ver si hay que poner notas en los preprints. https://tex.stackexchange.com/questions/219189/how-to-cite-an-unpublished-preprint-with-bibtex}
\clearpage

\end{document}






% \appendix
% \section{Anexo 1: Funcionamiento de la atención en los \textit{transformers}}\label{anexo1}
% \todo[inline]{Adaptar la parte de atención de este post: \url{https://jalammar.github.io/illustrated-transformer/}}

% \clearpage
% \section{Anexo 1: Resultados gráficos de las pruebas}\label{anexo1}
% \begin{figure}[H]
% \centering
% \includegraphics[width=0.9\linewidth]{imagenes/resultados2.png} 
% \captionsetup{width=.8\linewidth}
% \caption{Resultados de \textit{DPT} y \textit{AdaBins} en dos imágenes. Primera fila - imágenes de entrada, segunda fila - resultados \textit{DPT}, tercera fila - resultados \textit{AdaBins}.}
% \label{fig:resultados-anexo2}
% \end{figure}






% \clearpage
%\section{WIP: Texto de dataset mix5 y mix6}\label{unk}
%DPT parece que no está preparado para pasarse a onnx (o sí https://github.com/isl-org/DPT/issues/42), una de las opciones sería reescribir el modelo (probablemente simplificado) y reentrenar. Se describe a continuación los dataset empleados, reentrenar no parece muy viable. Otra opción que se podría explorar es destilar el modelo de alguna forma. También se puede leer los data efficient transformers de facebook a ver si reducen la necesidad de emplear tantisimas imágenes. Otra opción (si la implementación de Adabins está hecha orientada a onnx, sería usar solamente adabins y hacer todo el tfm con esa arquitectura).
%
%El dataset de profundidad que se usa para entrenar DPT (MIX6) es una ampliación de MIX5, presentado en \url{https://arxiv.org/abs/1907.01341}, el cual incluía los datasets:
%\begin{itemize}
%\item ReDWeb (\url{https://sites.google.com/site/redwebcvpr18/})
%\item DIML (\url{https://dimlrgbd.github.io/})
%\item 3D Movies (\url{https://github.com/isl-org/MiDaS/issues/13}) (\url{https://github.com/isl-org/MiDaS/issues/24}) el material complementario no está en el paperv3, ver v2 \url{https://arxiv.org/pdf/1907.01341v2.pdf}
%\item MegaDepth (\url{https://www.cs.cornell.edu/projects/megadepth/})
%\item WSVD (este también es un jaleo hacerlo \url{https://sites.google.com/view/wsvd/home} ; \url{https://github.com/aycatakmaz/wsvd_dataset_loader}) se descargan vídeos de youtube y se calcula, parecido a 3d movies, parece más automatizado.
%\end{itemize}
%
%Para el artículo de DPT, se incluyen cinco dataset más para conseguir MIX6:
%\begin{itemize}
%\item TartanAir (\url{https://theairlab.org/tartanair-dataset/})
%\item HRWSI (\url{https://github.com/KexianHust/Structure-Guided-Ranking-Loss})
%\item ApolloScape (\url{http://apolloscape.auto/stereo.html})
%\item BlendedMVS (\url{https://github.com/YoYo000/BlendedMVS})
%\item IRS (\url{https://github.com/HKBU-HPML/IRS})
%\end{itemize}
%
%Estos datasets se usan para entrenamiento solamente. Para el test, se usan los siguientes datasets: 
%
%\begin{itemize}
%\item DIW (\url{http://www-personal.umich.edu/~wfchen/depth-in-the-wild/})
%\item ETH3D (\url{https://www.eth3d.net/datasets})
%\item Sintel (\url{http://sintel.is.tue.mpg.de/about})
%\item KITTI (\url{https://stackoverflow.com/questions/63512296/kitti-eigen-split} - \url{http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction})
%\item NYU (\url{https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html})
%\item TUM (\url{https://vision.in.tum.de/data/datasets/rgbd-dataset}). 
%Tanto en el paper de DPT como en el paper de MiDas (versión anterior y convolucional de DPT (\url{https://github.com/isl-org/MiDaS})).
%\end{itemize}








%\section{Evaluación y pruebas}\label{resultados}
%% \todo[inline]{Hablar de las pruebas que se han hecho, valorar CPU GPU y TPU?. Comparar con los resultados que proporcionan los papers si es que los proporcionan.}
%% \todo[inline]{Hacer un experimento de pruning como el de Learning both weights and connections for efficient neural networks. La visualización de la distribución de los pesos antes y después del pruning también es muy interesante. (TFM)}
%% \todo[inline]{Hacer experimento de ablación para ver que partes corresponden a un mayor incremento/decremento de la velocidad/accuracy. (TFM)}
%% \todo[inline]{Redactar todo esto mejor}
%
%Para tener una estimación de cuánto tardan en realizar inferencia los modelos de estimación de profundidad monocular no eficientes que emplean \textit{transformers} y aprendizaje supervisado (expuestos en la sección \hyperref[aprendizaje-supervisado]{Aprendizaje supervisado}), se llevan a cabo una serie de pruebas en GPU y CPU. Siguiendo la metodología expuesta en \cite{visiontransformersDPT}, se mide el tiempo de inferencia media en 400 imágenes de 384x384 píxeles. Estas pruebas, se llevan a cabo tanto como para \textit{Dense Prediction Transformers} como para \textit{AdaBins} (Tabla \ref{tab:resultados-inferencia}). Además de estas pruebas de velocidad, también se han realizado pruebas sobre imágenes propias, algunas de las cuales están disponibles en el \hyperref[anexo1]{Anexo 1}. 
%% \todo{Incluir imágenes}.
%%Se obtienen resultados bastante razonables considerando que se emplean gráficas diferentes.\\
%
%\begin{table}[H]
%\centering
%\begin{tabular}{cccccc}
%\toprule
%             & \multicolumn{2}{c}{Inferencia (ms)} & \multicolumn{2}{c}{FPS} &           \\ 
%Prueba       & DPT  & AdaBins                      & DPT  & AdaBins          & Hardware  \\ \midrule
%Paper DPT    & 38   & -                            & 26.3 & -                & RTX 2080 (GPU) \\
%Propia   & 41   & 105                          & 24.5 & 9.52             & RTX 3070 (GPU) \\
%Propia (Colab) & 60   & 164                            & 16.7 & 6.08                & Tesla T4 (GPU)\\
%Propia    & 1675 & 2008                         & 0.60 & 0.50             & AMD\textsuperscript{\textregistered} Ryzen 7 3800x (CPU) \\ \bottomrule
%\end{tabular}
%\caption{Resultados de tiempos de inferencia con DPT y AdaBins}
%\label{tab:resultados-inferencia}
%\end{table}